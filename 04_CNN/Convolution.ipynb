{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python - libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from operator import methodcaller\n",
    "\n",
    "if not str(Path(os.getcwd()).parent) in sys.path:\n",
    "    sys.path.append(str(Path(os.getcwd()).parent))\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "## Image\n",
    "import requests\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# helper module\n",
    "try:\n",
    "    import my_helper as helper\n",
    "except :\n",
    "    print(\"[INFO] Downloading helper function\")\n",
    "    import requests\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/Lashi0812/PyTorch2/master/my_helper.py\")\n",
    "    with open(\"my_helper.py\",\"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    import my_helper as helper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlation\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X: torch.Tensor, K: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute 2D cross-correlation\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros(size=(X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i : i + h, j : j + w] * K).sum()\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(0,9,dtype=torch.float).reshape(3,3)\n",
    "K = torch.arange(0,4,dtype=torch.float).reshape(2,2)\n",
    "H = corr2d(X,K)\n",
    "H"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self,kernel_size) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self,x:torch.tensor):\n",
    "        return corr2d(x,self.weight) + self.bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6,8))\n",
    "X[:,2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK8AAACICAYAAACRO7v7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABtElEQVR4nO3UsQnCYBRGURMcQuzt3cKZ3cLeXpzC3wmEEAnhwjn1K77i8qYxxjhA0Lz3AFhLvGSJlyzxkiVessRLlnjJEi9Z4iXruPTw875sueMvt/N17wk599dj7wk/zafnsruNd8BmxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMkSL1niJUu8ZImXLPGSJV6yxEuWeMmaxhhj7xGwhs9LlnjJEi9Z4iVLvGSJlyzxkiVessRL1heuVQ8JzgMdkwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(X)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_detector = torch.tensor([[1.0, -1.0]])\n",
    "edge_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = corr2d(X,edge_detector)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK8AAACYCAYAAACS7blgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAB7UlEQVR4nO3UwWlCURBAURUbkaQIO7EFd0GsI4Ts0oKdWISSUp4tyIfH58I562GYxWW2Y4yxgaDd2gfAUuIlS7xkiZcs8ZIlXrLES5Z4ydq/O3j4/Z5ywPP0N2Xvx+08ZW/R5+U+Ze/j5zhl7//X9a05n5cs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES5Z4yRIvWeIlS7xkiZcs8ZIlXrLES9Z2jDHWPgKW8HnJEi9Z4iVLvGSJlyzxkiVessRLlnjJegGaqRIpTiQRmwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(H)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the kernel\n",
    "\n",
    "If you want we looking then we define kernel matrix like sobel edge detector.But the hard to define the kernel for the deep layer , so why cant the neural network learn the kernel.\n",
    "\n",
    "Let try find kernel that produce H from X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "conv2d = nn.LazyConv2d(out_channels=1,kernel_size=(1,2),bias=False)\n",
    "\n",
    "X = X.reshape((1,1,6,8))\n",
    "H = H.reshape((1,1,6,7))\n",
    "lr = 3e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 , loss =8.64\n",
      "epoch 4 , loss =1.99\n",
      "epoch 6 , loss =0.56\n",
      "epoch 8 , loss =0.18\n",
      "epoch 10 , loss =0.07\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    H_hat = conv2d(X)\n",
    "    l = (H_hat - H)**2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i+1) %2 == 0:\n",
    "        print(f\"epoch {i+1} , loss ={l.sum():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9609, -1.0127]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1,2))\n",
    "\n",
    "#? this is same kernel we used earlier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "\n",
    "In general, if we add a total of $p_h$ rows of padding\n",
    "(roughly half on top and half on bottom)\n",
    "and a total of $p_w$ columns of padding\n",
    "(roughly half on the left and half on the right),\n",
    "the output shape will be\n",
    "\n",
    "$$(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1).$$\n",
    "\n",
    "This means that the height and width of the output\n",
    "will increase by $p_h$ and $p_w$, respectively.\n",
    "\n",
    "In many cases, we will want to set $p_h=k_h-1$ and $p_w=k_w-1$\n",
    "to give the input and output the same height and width.\n",
    "This will make it easier to predict the output shape of each layer\n",
    "when constructing the network.\n",
    "Assuming that $k_h$ is odd here,\n",
    "we will pad $p_h/2$ rows on both sides of the height.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_conv2d(conv2d:nn.Module,X:torch.Tensor)->torch.tensor:\n",
    "    \"\"\"Return output of conv2d layer by modify the input shape [b,c,h,w]\"\"\"\n",
    "    X = X.reshape((1,1)+X.shape)\n",
    "    Y = conv2d(X)\n",
    "\n",
    "    return Y.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((8,8))\n",
    "conv2d = nn.LazyConv2d(out_channels=1,kernel_size=3,padding=1)\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.LazyConv2d(out_channels=1,kernel_size=(5,3),padding=(2,1))\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "$$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.LazyConv2d(out_channels=1,kernel_size=3,padding=1,stride=2)\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.LazyConv2d(out_channels=1,kernel_size=(3,5),padding=(0,1),stride=(3,4))\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi input Channel\n",
    "\n",
    "Number of Kernel channel should be same as the input channel\n",
    "\n",
    "kernel of shape --> [Cin * Kh * Kw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 2.],\n",
       "         [3., 4., 5.],\n",
       "         [6., 7., 8.]],\n",
       "\n",
       "        [[1., 2., 3.],\n",
       "         [4., 5., 6.],\n",
       "         [7., 8., 9.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_channel0 = torch.arange(0,9,dtype=torch.float).reshape(3,3)\n",
    "X_channel1 = torch.arange(1,10,dtype=torch.float).reshape(3,3)\n",
    "X = torch.stack((X_channel0,X_channel1))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [2., 3.]],\n",
       "\n",
       "        [[1., 2.],\n",
       "         [3., 4.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_channel0 = torch.arange(0,4,dtype=torch.float).reshape(2,2)\n",
    "K_channel1 = torch.arange(1,5,dtype=torch.float).reshape(2,2)\n",
    "K = torch.stack((K_channel0,K_channel1))\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X,K):\n",
    "    return sum(corr2d(x,k) for x,k in zip(X,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in(X,K)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Output channel \n",
    "\n",
    "Now we stack number of kernel with same input channel as the number of output channel \n",
    "\n",
    "single kernel of shape --> [Cin * Kh * Kw]\n",
    "\n",
    "Final kernel shape after stacking --> [Co * Cin * Kh * Kw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X,K):\n",
    "    \"\"\"kernel shape --> [Co * Cin * Kh * Kw]\"\"\"\n",
    "    assert X.shape[0] == K.shape[1] , \"Kernel should have same number of channel as input channel\"\n",
    "    return torch.stack([corr2d_multi_in(X,k) for k in K],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2, 2, 2]), torch.Size([2, 3, 3]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 different output channel\n",
    "K = torch.stack((K,K+1,K+2),dim=0)\n",
    "K.shape ,X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X,K)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAACbCAYAAADskKVVAAAa2UlEQVR4nO3deVRU590H8O8srCqiENRhD+IGriC4IEHjWlLcoi9NGmPbELPYbC55076m9dX2zaKJ1djUnjTaxpoo7gbXShTQWBURDYgooqhBURhgZJ3lef/gOCc0icwduQOX+X7O8Rxh7nN/v5m5zHfm3mfuVQkhBIiIiBRI3dYNEBER2YshRkTkINzx1fo6dIipVCqoVCrZ69y5cwdZWVmy16GHV1VVhUGDBuHcuXPNfp+amoohQ4bgyJEjDu3HUdsotZ3CwkI899xz8PLyglarRXBwMFasWAGTyWTzOgoKCpCfn98q/Xz55ZcwGo2tsq72oEOHmKPMmzcPBw8ebOs2yAZmsxnnz59HXV2d9Xepqan4+c9/jtdffx0JCQlt2B11NPn5+RgxYgSqqqqwb98+XLlyBR988AE++ugjTJkyBRaLxab1TJ48GYWFhQ/dz8WLF/HTn/4UZrP5odfVXjhNiFVWVuLatWtobGzEoUOHkJGR0eydUGFhIerq6nDq1Cns27cPFRUV1tsqKipQXFzcbH0XLlxATU0Nbt26hcrKSpSVlaGkpMRh94daR2pqKp555hmsX78ezz77bLPbysrKkJaWhjNnzjTbDXTt2jVUVlbixIkTyMnJQV1dHQoLCyGEQGZmJg4dOtQsJAHAYrHg2LFj2LdvHwwGg0PuG7W9V155BRMnTsSWLVswevRoBAcHY+bMmcjKysLXX3+NjRs3AgBu3ryJ0tJS67jGxkbk5eXBbDajuLgY9fX1KCkpQVlZGaqqqnD16lVUVlZi7969OHnyZLPts7CwsNk2VlFRgStXrsBisViDMD8/H7W1tQ56FGQmOjAA4v5d/Pvf/y4GDx4sBg0aJMaNGyf8/PxEdHS0aGhoEEIIERwcLMaMGSNGjRolpk+fLnx8fERGRoYQQoi1a9eKMWPGNFt3ly5dxFdffSVWrVolvL29hb+/v1i4cKFj7yBJVl5eLgCIEydOiC1btgh3d3eRmpr6veXef/994evrK5KSkkR4eLgYP368qKmpEUII8ZOf/ESMHTtWeHh4iB49eogTJ04IPz8/ER8fL+Lj48Wjjz4q/P39RWlpqRBCiKtXr4r+/fuL6OhoMXnyZNG9e3exZ88eIUTzbZQ6ltLSUqFWq0V6evoP3p6SkmJ9XXnuuefEK6+8Yr3t8uXLAoCorKwUL730knBxcRF9+vQRf/3rX8WmTZtE3759RVhYmEhOThZ9+vQR06ZNE0ajUQghRGBgoEhLS7Oua+3atWLkyJGisbFRDB48WAAQMTExIjc3V8Z77zgd+q/nP0MMgDhz5owQQojbt28LV1dX6wYWHBwsEhMThdlsFkII8c4774jIyEhhsVgeGGJCCDF16lSxZMkSR90tegj3Q2zBggVCq9UKPz8/cfv27WbL5ObmCk9PT3H+/HkhhBBGo1GMGzdOLFu2TAjRFGIRERHCYDCImpoakZ2dLQCIjRs3CiGEqK+vFyEhIWLNmjVCCCESExNFSkqKsFgsQggh9u/fL7y9vUV9fT1DrAM7ePCgACAMBsMP3r5mzRoREBAghHhwiAnRFEw7duwQQgixadMmAUAcOXJECCFEZWWlCAoKEp999pl12R8KMSGEuHDhggAg6urqWvneth2n2Z0IAKGhoRg6dCgAwM/PDz169IBer7fe/uqrr0KtbnpIfvWrX+Gbb77B9evX26RXktef//xn7N69GzqdDnPnzm12bGLnzp0ICAhAQUEBtm7dip07dyIoKAh79uyxLpOYmIjOnTvD09PT+rsZM2YAANzc3NCvXz/o9XrU1tbiwIEDCAgIwLZt27B161ZUV1ejtraWk4GchEaj+dHfCztnKw4bNgyPPfYYAKBr166YPXs20tLS7O5RybRt3YAjffcFB4A1sO7z8vL63rI1NTUAvj811t6Nj9qH9evXY8qUKQgODkZUVBRWrVqFN954AwBQXl6OmpoapKamNhszcuRI6/87d+78vXV6eHhY/39/26qurobJZMKJEyeQl5dnvX3atGnf2x6pYwkNDQUA5OTkYNSoUd+7/dSpUwgODrb+/N3XlJZeX777WgU0bY/3X6ukrkvpnCrEWvLdA6slJSVwcXGBTqeDSqVCQ0OD9ba6urqOc1DUSYWEhAAABgwYgHfffRcLFy5EfHw8oqOjERwcDJ1Oh82bN1uXz8nJQXV1tfVnW6fFd+/eHZ07d8aCBQvw+OOPAwBMJhO2bNmCPn36tOI9ovamd+/eiI+Px0cffYSRI0c222Zu3ryJ7du34/333weA773GlJeXP3DdpaWlEEJY15mXl2cNTanrUjqn2p3YkrfffhulpaWorq7G/PnzkZycjK5duyIkJATnzp3DyZMnUVlZid/+9rfNxrm6uqKwsBBXr15to87pYcyfPx8JCQlITk5GdXU1Zs2ahW+++QYffPABjEYjiouLMXv2bLu+RuHq6oqf/exneO2111BSUgKTyYTly5fjpZdeglbL95Ad3apVq5CWloann34ap06dwo0bN/DFF18gISEBERER+MUvfgGg6U3VwYMHcfXqVZSWluKPf/xjs/W4uroiNzcXt27dAtA0VX716tUwmUw4cOAA9uzZg5SUFOu6vvjiC1RVVSEnJwcbNmxoth4AOH78OO7du+eIh0B2ThNiXbp0QVBQULPfhYSEoFOnTtafhw4diiFDhsDX1xd+fn5YvXo1AGDSpElISUnBY489hoCAAHTr1g1PPPGEdffRnDlzkJGRgXnz5jnuDpFd1Go1evfuDTc3t2a/27BhAzw9PbFy5UoEBgbiyy+/xObNm+Hu7o6BAwciKSkJS5cuBQDodDp069bNOt7NzQ29e/duVken06F79+4Aml7I4uPjERkZCQ8PD+zatQuHDx9G165dHXCPqS0NHToUOTk5MJvNGDt2LIKCgrBo0SLMnTsX6enp1jcyL7/8MgYNGoSwsDBERUUhOTkZvXv3tu6WfvHFF7F69WqsXLkSABAUFITMzEy4u7sjJSUFGzZsQGRkJADgww8/REFBAby9vTFv3jwsWLAAAQEBAIDg4GAkJSVh5syZyMzMbINHpPWpREffYWqjkJAQrFq1ClOnTgXww7uL7j9UPMOC87BYLN87dmov0TQbuNXWR8rT0vZky/b2+eefY+nSpSgoKHjg8q257bZn3J/xHx4UUAwv59OaLwI8xRS1tD1J3d4etLwzBBjgRLsTW/Liiy8iPDy8rdsgInqg/v37W49/EXcnEhGRgvGTGBERKRZDjIiIFIshRkREisUQIyIixWKIERGRYjHEiIhIsRhiRESkWAwxIiJSLIYYEREpFkOMiIgUiyFGRESKxRAjIiLFYogREZFiMcSIiEixGGJERKRYDDEiIlIshhgRESmWtq0b+K5r167h6NGjDql1+/ZtaLVa+Pj4yF7r22+/haenJ7y9vWWvBQCJiYkOuV/OLD09HTdu3HBIrU6dOmHq1KnQatvVnytRu6ASQoi2buK+5ORkbN68ua3bULy5c+di/fr1bd1Gh2UwGODl5eXQmv/85z/x1FNPObQmkRK0q7d2NTU16OLtgxWp/5a91suJEfDv1ROHDx+Wvdbw4cNhMpmQk5Mje60BAwbAZDLJXseZNTY2AgCSnn0NiT+fL2ut2zeK8fYvJvA5JfoR7SrEAMBPF4xeQWGy13F1c0doaCjCwuSvFRgYiNu3bzusFjlGz6Aw2bdVlYqHrYkehH8hRESkWAwxIiJSLIYYEREpFkOMiIgUq91N7CAiai8KCgpw8eJFh9S6fv06evbsCRcXF9lrlZSUQKfTOeS7h25ubpgwYQI0Go0s62eIERH9iKlTp6KwsNAhtdRqNSwWi0NqOdpf/vIXzJs3T5Z1M8SIiH5EeXk5YmJi8PHHH8teKyoqCuPHj8e7774rax2TyYTY2Fj0798fGzdulLWWXq/H+PHj0dDQIFsNhhgR0Y/QarUYNmwYhg0bJnsttVrtkFr3vziv0+lkr6XX62VdP8CJHUREpGAMMSIiUixZdifW1NRg4sRJuF1WBrXa9py8VVqKRqMJL07uZ/MYk9EIg/4OvLr5QOviZvM4Y30tTp8+jX79bK/V2NiIu3fvwtfXF66urjaPKykpgdlsllTLYrHA398fO3bscNjZ753R3r178drrb0gaYzGbodZosOXjPyBt40e2j7NYMCxuEua9vUZqm0T0I2QJseLiYhw/fgyubh6IGfdTm8f1CBssudb5k0dRW2OAp4cb4kfF2D5wuPRa6enpMBgM6NSpE4YPH27zuMGDpdfas2cPLl26hJs3bzLEZHT48GFcKryIuCmzJY3r1Uf6sYSsfVtwrMbAECNqRbJO7Hjj/c8watJMOUtg3xfr8PHvXsCmTZswYcIEWWsdPHgQkyZNwtKlS/H888/LWmvr1q2YNWuWrDWoiUenLli8Sv5LAHl09kL20X2y1yFyJjwmRkREisUQIyIixWKIERGRYjHEiIhIsRhiRESkWDztFBF1eEVFRXjmmTkwGo1QqVQ2j6vQ67F9+w5kZ5+xeUxNTQ3KysoQEhIs6XuyAPDZZ5/hyJEjNi9/79493L17F0FBQTbXEkJApVLh9OnTiI2NtbmWEAKxsbFYs6Z9fUWEIUZEHd7Ro0fx9dfHERg2AL69Am0eFxkzFgBgklAr/9RJAEBY2KOSvuNpz1eETp5sqjVw4EBJl1WZOHGi5FoHDhxATk4OQ4yIqK38/m/78YiEELPHH16ajn8f3on9+/fLfqKC5cuXY8mSJdi7d6+kswjZY/HixVi7dq2sNezBY2JERKRYDDEiIlKsFncnlpWVITMzU9JKS0pKAAAFZ09IGmeoqgCEQBdvH5vHFOU3HXDNzMxEdXW1zePu3LkDFxcXSR/3z549CwA4c+YMtm3bZvM4AEhISICPj+33674VK1bA19fX5uWLiooQFRUluU5HcOrUKeu2Z6vCwkKYTUYcPyDt+XRxc8ewuEnQSLy8u7GxXlKt2zeKJa2fyNm0+Bf49ttvY926dXatfOenK+waZ49ly5Y5rNa6deskPyZPPvkkUlNTJdfasGGD5DH5+fmSxyid0WjEmDHxaGiot2v8O688KXnMS/+7DpP/S9o5NGsMlXbV6tWrl+QxRM6gxRAzGAwAgNW7z8nezMLZsdCqVThxQtonOHuMHz8eer0e2dnZstcaMWIEGhsb7Rq7fft29O7d2+blBw0ahIEDB9pVS8ksFgsaGuoR/8RTePL5/5a1lqGyHL+dMxYmo/Tn1MfHB4f/9S9JY7RaraTL+BA5E5v2heiCwxHSV/4XRo9OXRASoHPIi3D//v2Rm5vrkFp9+/a1e2yfPn0QERFh8/JhYWHQaDR211O6kD4DZd9Wq/Xldo/Val2c8k0GkVw4sYOIiBSLIUZERIrFECMiIsViiBERkWLxtFNERGQTk8mE9957z+bl789ut1gscrXEECMiItuYTCa8+eabksfl5eXJ0E0ThhgREdnE1dUVhw4dsnl5vV6PpKQkSV8TkoohRkRENlGr1YiLi7N5eb1ebx0nF07sICIixWKIERGRYrXD3YmirRtodWazGWlpaejcubPNY+rq6qBSqXDv3j0ZO6OHse2v7+JfWz+1efkrF3Lg16OnjB0ROZ92GGIdj0qlgq+vL4YPH27zmLS0NAgh4O7uLmNn9DB6+nZFaKi/zcsPCPPH2LFjZeyIyPm0uxBTqVRt3UKrU6vViI2Nxa5du2wes3XrVsyaNQtaiderIsd54YUXMH/+/LZug8ip8ZgYEREpFkOMiIgUiyFGRESKxRAjIiLFYogREZFiceobtZqqqiosWbIEoaGheP3119u6HSKru3fvAgBenNQXLq5ustaqMVQCAAIDA2WfXVxZ2VTL19cXGo1G9lrtcfY4Q4xajdlsxpo1azBhwgSGWAd15MgR7N69W9KYX/7yl4iMjJSpI9sYjUYAgK5XDwQEBMhaKysrCwAQEhICb29vh9QaMGAAXFxcZK/FEKMOx2w249NPP8XMmTPh7e0NFxcX6HQ6AE2XX7h48SJmzJjRxl1Sazlz5gw+/PBDSWMSEhLaPMR69eoFAMjIyEBgYKCstaZPn46dO3ciMzNT9hBbvnw5lixZgoyMDLi6uspaa/HixVi7dq2sNezBEKOHcvz4cTz//POYP38+nnnmGXTu3Bnffvst4uLicOzYMfj4+CApKYlf2u4g+vbti+TkZElj/P1tP6sJkVR8ZaGH4u3tjV//+tc4duwY/vGPf8BoNOLQoUPw9PREVFQUJk6ciNraWnh5ebV1q9QKEhMTkZiY2NZtEFm1GGINDQ349tolbP7zctmbqSq/g7PlZVi+XP5aR48eBQCH1MrJybHuzpAqKysL169ft3n5oqIiSedofFgDBw7E6tWr0djYiBkzZmD//v0wm81YuXIlXnjhBYf1cV925n6E9h8iaw1DZTkAID8/H/v375e1FgB06tQJcXFx7fJ4BFGbEy2IiIgQaDq1PP89xL/Q0NCWHupmUlNT7a4VGRkpqdbDslgsIiUlRQAQK1euFFFRUQKA2LZtm8N6MBqNwt3dw4HPqcqh28+WLVsc9lh2RH/7298EAFFSUiJ7rWnTpgkAQq/Xy15r2bJlAoBoaGiQvdaiRYuEp6enpDEVFRUCgPjTn/4kU1dCtPhJLDIyEnl5eTh+/HhLiz60hIQEGI1GHDt2TPZaEydORF1dHTIzM2WvNWrUKPTr18+uscPm/A6dfHU2L5/5wTxZLwX+Q06dOoVPPvkE4eHhePnllzF69GiMGjUKr776KiZPngxPT0/Ze9Bqtbh8+RJKSkpkr1VVVYUpU6ag9+NPIWK6vCcANpQWI/0PT6Ourk7WOrbatWsX1q9fL2nMb37zG8TExMjUETm7FkNMo9EgPDwcI0eOlL0Zb29v6HQ6h9SKiopCbm6uQ2oNHTrU7umvjybMQvdQ20Pp/Ob3ZJ9q+59iYmKQnp4ODw8PuLm5ITY2FuvXr8e4ceMcEmD3+fv7O2QSQXl50+5Ev4iR6BEh7/bj0b19XX+sqKhI0tUYgKYp9kRy4cQOahUJCQnNfp4zZ04bdUJyiomJwVtvvSVpTHh4uEzdEDHEiEiCuLg4xMXFtXUbRFY8dyIRESkWQ4yIiBSLuxOJiMgmFotF0kx1vV4PABBCyNUSQ4yIiGzT2NiI0aNHSx6Xl5cnQzdNGGJERGQTlUaL4b/8vc3LG2sNOLvp/9C/f3/ZemKIERGRTdQaLYY+bftXLBoMepzd9H+yXuuMEzuIiEixGGJERKRY7W53opyzWNqKxWJBVlaWpC+J3j9/pLG+Rq62CE1XM/if/1kiabszmZquEnw+9UNcSd9s8zgBAf+o8Yiea/sxBSJ6sHYXYh2REAL6qmqcypU+Q0ejlfdqrc5u9+7dyMrKhG7IWAmjXCQu3+Tbs1+h+mYRQ4yoFbW7EOuI10xSqTUIiv0JJv3B9hOnXjm6Ff/6/SyotY49ma8zcvXsgic+TJe9Tsb7KSg5uU/2OkTOhMfEiIhIsRhiRESkWAwxIiJSLIYYEREpFkOMiIgUq93NTiQiam0VFRUAgCeeeAKBgYGy1kpLS7PW8vb2dkitpKQkaLXyvpynpaVBpW5/n3sYYkTU4dXX10OlUqGgoABFRUWy1lKpVFCpVMjOzpb1nIEAoFarIYTA0aNHHVNL1gr2YYgRUYen0+kghMDly5dl/yQ2ffp07Ny5E6WlpbJ/Elu+fDmWLFmCqqoquLrKe2KExYsXY9WatbLWsEf7+2xIRERkI4YYEREpVou7E41GIy5duoTMzEzZm9Hr9SgrK3NIrZMnT6KhocEhtXLP5iAwtpddY+8WZqPBUGHz8pU3iwCMtKuW0l26dAm3bt2SNObGjRuwmE0oPSdtO9C4uuORvtGST5NmMTZKqmUoLZa0fiJn02KI5efnAwDi4+Nlb+a+jlirsuSCXeOOvPOs5DHdu3e3q5aSmUwmREUPh6G6yq7xe16Vvh0kvLkBfSZLe34aDBV21XLG55TIFi2GWEREBPLy8vDee+/J3sybb74JIYRDar311lswm82InSd/rX+vW4yuAX3tGvvJJ58gJCRE0pjo6Gi7aimZ2WyGoboK/sPGY8hT/y1rrfrqchz+3/9CY51B8thu3bphy+YvJI3RarUYM2aM5FpEzqDFENNqtQgPD8eiRYtkb2bFihXQ6XQOqZWWloavT+dicLL8tS4f/hwaF/tmDo0YMQIRERGt3FHH5R/1OPyjHpe1Rn1Vud1jXV3d8Pjj8vZH5Ew4sYOIiBSLIUZERIrFECMiIsViiBERkWLxtFNERGQTYbHg7uWzNi/faNDL2E0ThhgREdnEYmrE9pShksd5eXnJ0E0ThhgREdnExcUFn3/+uaQxGo0GkydPlqkjhhgREdlIo9Fg+vTpbd1GM5zYQUREisUQIyIixWp3uxOFaI/XDn04QlhwO+9r7F00yeYxN04flLEjag35O9ai5Pgem5e/cfogHunRU8aOiJxPuwuxjkgFAXeVEb3Utp9hvefwGPg98gjCwsJk7Iwehpe6Dj0kPqcTJ0yQsSMi59PuQkzq9ZmUQKXWID4+Hrt27WrrVqgVLVq4EPPnz2/rNoicGo+JERGRYjHEiIhIsRhiRESkWAwxIiJSLIYYEREpVrubnUhEJJfZs2cjMDBQ1ho7d+601vL29pa1VmpqKgAgOTkZWq28L+epqamy17BH++uIiKiVRUVFYdiwYaioqEBlZaWstYKCglBVVYVr167h+vXrstYKDAyEwWBAXl4e1Gp5d6z17dsXI0eOlLWGPRhiRNThDR48GNnZ2W3dBsmAx8SIiEixGGJERKRYNu1OvHTpkkM+ildXV0Ov1zuk1rlz52Csv4c7F+WvVXHlG6C/vAeTqcmdi9myP6cN1eWyrp+IbNdiiLm7uwMAoqOjZW/mPkfW2vGCY2p17drVIXWclVqthlqtQXHGVhRnbHVITT6nRG1PJVq49sm9e/fw1VdfOaSZe/fuoba2Fn5+frLXqq6uhtFohI+Pj+y1ACAmJgY9evRwSC1ndeHCBVy+fNkhtTQaDSZMmAAXFxeH1COiH9ZiiBEREbVXnNhBRESKxRAjIiLFYogREZFiMcSIiEixGGJERKRYDDEiIlIshhgRESkWQ4yIiBSLIUZERIr1//V0mVdE0ZtlAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1x1 Convolution kernel\n",
    "\n",
    "Even though it loss the ability of Locality information, But if gather the information of single pixel present across the channel.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "Input and output have same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X,K):\n",
    "    c_i ,h,w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i ,h*w))\n",
    "    K = K.reshape((c_o,c_i))\n",
    "    Y = torch.matmul(K,X)\n",
    "    return Y.reshape((c_o,h,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(size=(3,3,3))\n",
    "K = torch.randn(size=(2,3,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 3]), torch.Size([2, 3, 3]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1 = corr2d_multi_in_out_1x1(X,K)\n",
    "Y2 = corr2d_multi_in_out(X,K)\n",
    "Y1.shape ,Y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1871, -1.0291,  1.8334],\n",
       "         [ 0.6520, -2.2396,  2.6665],\n",
       "         [-0.6781,  0.7516, -0.5707]],\n",
       "\n",
       "        [[-0.8911,  1.8803, -0.3313],\n",
       "         [-0.9741,  1.5813,  0.1778],\n",
       "         [-1.7064, -0.2767,  0.5998]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1871, -1.0291,  1.8334],\n",
       "         [ 0.6520, -2.2396,  2.6665],\n",
       "         [-0.6781,  0.7516, -0.5707]],\n",
       "\n",
       "        [[-0.8911,  1.8803, -0.3313],\n",
       "         [-0.9741,  1.5813,  0.1778],\n",
       "         [-1.7064, -0.2767,  0.5998]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert float(torch.abs(Y2-Y1).sum()) < 1e-6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X,pool_size,mode=\"max\"):\n",
    "    h,w = pool_size\n",
    "    Y = torch.zeros(size=(X.shape[0]-h+1,X.shape[1]-w+1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            assert hasattr(torch,mode) , \"try max or mean for pooling\"\n",
    "            f = methodcaller(mode,X[i:i+h,j:j+w])\n",
    "            Y[i,j] = f(torch)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(0.,9.).reshape((3,3))\n",
    "pool2d(X,(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X,(2,2),mode=\"mean\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
