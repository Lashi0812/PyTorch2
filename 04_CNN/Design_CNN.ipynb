{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import os\n",
    "import sys\n",
    "import dataclasses\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from operator import methodcaller\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from typing import (\n",
    "    List,\n",
    "    Tuple,\n",
    "    Dict,\n",
    "    Any,\n",
    "    Mapping,\n",
    "    Callable\n",
    ")\n",
    "from enum import Enum\n",
    "# adding the path\n",
    "if not str(Path(os.getcwd()).parent) in sys.path:\n",
    "    sys.path.append(str(Path(os.getcwd()).parent))\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import (\n",
    "    nn,\n",
    "    Tensor\n",
    "    )\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "try:\n",
    "    from torchmetrics import Accuracy\n",
    "except:\n",
    "    print(f\"[INFO] Installing the torchmetrics\")\n",
    "    %pip install torchmetrics\n",
    "    from torchmetrics import Accuracy\n",
    "\n",
    "try:\n",
    "    import torchinfo\n",
    "except:\n",
    "    print(f\"[INFO] Installing the torchinfo\")\n",
    "    %pip install torchinfo\n",
    "    import torchinfo\n",
    "\n",
    "# helper function\n",
    "try:\n",
    "    import my_helper as helper\n",
    "except:\n",
    "    print(\"[INFO] Downloading the helper function from github\")\n",
    "    import requests\n",
    "    response = requests.get(\"https://raw.githubusercontent.com/Lashi0812/PyTorch2/master/my_helper.py\")\n",
    "    with open(\"my_helper.py\" ,\"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    import my_helper as helper\n",
    "\n",
    "\n",
    "## Connect Persistence memory\n",
    "try :\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Paths\n",
    "    DRIVE_PATH = Path(\"/content/drive\")\n",
    "    MODEL_SAVE_PATH = Path(\"/content/drive/Othercomputers/My PC/drive/models\")\n",
    "\n",
    "    # mount drive\n",
    "    drive.mount(str(DRIVE_PATH))\n",
    "except:\n",
    "    MODEL_SAVE_PATH = Path(os.getcwd())/\"models\"\n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Design\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNeXt Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeXtBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_channels, groups, bot_mul, use_conv1x1=False, stride=1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        bot_channels = int(round(num_channels * bot_mul))\n",
    "        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv2 = nn.LazyConv2d(\n",
    "            bot_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            groups=bot_channels // groups,\n",
    "        )\n",
    "        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "        self.bn3 = nn.LazyBatchNorm2d()\n",
    "        if use_conv1x1:\n",
    "            self.conv4 = nn.LazyConv2d(\n",
    "                num_channels, kernel_size=1, stride=stride, padding=0\n",
    "            )\n",
    "            self.bn4 = nn.LazyBatchNorm2d()\n",
    "        else:\n",
    "            self.conv4 = None\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # passing through bottleneck ie project to lower dimension\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        # do the conv operation in low dimension and gather feature\n",
    "        y = F.relu(self.bn2(self.conv2(y)))\n",
    "        # projecting to original dimension\n",
    "        y = self.bn3(self.conv3(y))\n",
    "        if self.conv4:\n",
    "            x = self.bn4(self.conv4(x))\n",
    "\n",
    "        return F.relu(y + x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnyNet(helper.Classifier):\n",
    "    def __init__(self,arch:Tuple[Tuple[int,int,int,int]],stem_channels:int,lr:float=0.1,num_classes:int=10) -> None:\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.num_classes = num_classes\n",
    "        self.arch = arch\n",
    "        self.stem_channels = stem_channels\n",
    "        \n",
    "        self.net = nn.Sequential(self.stem(self.stem_channels))\n",
    "        \n",
    "        for i,s in enumerate(arch):\n",
    "            self.net.add_module(f'stage{i+1}',self.stage(*s))\n",
    "        \n",
    "        self.net.add_module(\"head\",nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),nn.Flatten(),\n",
    "            nn.LazyLinear(self.num_classes)\n",
    "        ))\n",
    "        \n",
    "    \n",
    "    def stem(self,num_channels:int)->nn.Module:\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(num_channels,kernel_size=7,stride=2,padding=3),\n",
    "            nn.LazyBatchNorm2d(),nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def stage(self,num_channels,depth,bot_mul,groups)->nn.Module:\n",
    "        blk = []\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                blk.append(ResNeXtBlock(num_channels,groups=groups,bot_mul=bot_mul,\n",
    "                                        use_conv1x1=True,stride=2))\n",
    "            else:\n",
    "                blk.append(ResNeXtBlock(num_channels,groups=groups,bot_mul=bot_mul))\n",
    "        return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf51ac970e033765f04eaa06b2d945d30ec4bd27f0fa0badbc17cd319e8357a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
